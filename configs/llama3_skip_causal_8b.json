{
  "_name_or_path": "meta-llama/Meta-Llama-3-8B",
  "sparsity": 0.3,
  "thresholds": [
    0.003071251092478633,
    0.0032092262990772724,
    0.004811936989426613,
    0.006146600935608149,
    0.008167627267539501,
    0.00941960047930479,
    0.010491803288459778,
    0.011232503689825535,
    0.011155850253999233,
    0.011817514896392822,
    0.011672375723719597,
    0.012277831323444843,
    0.013189301826059818,
    0.014693950302898884,
    0.015120591036975384,
    0.016914963722229004,
    0.018072837963700294,
    0.01880616322159767,
    0.018483471125364304,
    0.019643893465399742,
    0.019259095191955566,
    0.021713104099035263,
    0.021428998559713364,
    0.02187361940741539,
    0.02167775295674801,
    0.023116255179047585,
    0.02462640590965748,
    0.028577404096722603,
    0.03488597646355629,
    0.04226861521601677,
    0.06466718018054962,
    0.1539062112569809
  ],
  "architectures": [
    "LlamaSkipConnectionForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "llama-skip",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "vocab_size": 128256
}